<!doctype html>
<html lang="en">


	<head>
		<meta charset="utf-8">

		<title>Quantitative Methods For Binary Data - Rafael Turner</title>

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
	  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

		<link rel="stylesheet" href="css/reveal.min.css">
		<link rel="stylesheet" href="css/theme/default.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->


		<!-- If the query includes 'print-pdf', use the PDF print sheet -->
		<script>
			document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>


	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">

				<section>
					<h1>Quantitative Methods For Binary Data</h1>
					<h3>A Honey Badger's Journey</h3>
					<p>
						<small>Created by <a href="http://www.reddit.com/r/REMath/">Rafael Turner</a> / <a href="http://www.invincea.com/the-invincea-team/employment-invincea-labs-llc/">Invincea Labs</a></small>
					</p>
				</section>

		<section>
					<h3>Slide Navigation</h3>
      <ul>
         <li>Page Up</li>
        <li>Page Down</a> </li>

        </ul>
				</section>





		<section>
					<h3>Overview (Hit Escape)</h3>
      <ul>
         <li><a href="#/1"> Motivation</a></li>
        <li><a href="#/12"> Probability Primer</a> </li>
         <li><a href="#/15"> Instruction Concept Learning</a></li>
         <li><a href="#/17"> Sklearn & Clustering</a></li>
        <li><a href="#/22">Malheur & Dimensionality Reduction</a> </li>
         <li><a href="#/23">Future Work</a> </li>
         <li><a href="#/24">References </a></li>
          <li><a href="#/25">Contact</a></li>

        </ul>
				</section>



    <section>
       <p> "Over the past 2.5 years Endgame received 20M samples of malware equating to roughly 9.5 TB of binary data.
        In this, we’re not alone. McAfee reports that it currently receives roughly 100,000 malware samples per day and received roughly 10M samples in the last quarter of 2012.
         Its total corpus is estimated to be about 100M samples. VirusTotal receives between 300k and 600k unique files per day, and of those roughly one-third to half are positively identified as malware." </p>
      <p> - Zachary Hanif, Telvis Calhoun, Jason Trost </p>
     </section>
    <section>
        9.5 TB of binary data
    </section>

    <section>
			<a class="test">
				<img width="380" height="426" src="images/Honey-Badger_dont_care_500.png">
			</a>
    </section>

     <section>
			<a class="test">
				<img width="600" height="600" src="images/ref_matrix.png">
			</a>
      </section>



      <section>
        <p> Stuxnet variants were kicking it back in DBs in 2005. </p>
        <p> Hits the community in 2010.</p>
        <p> About a five year lag? </p>
      </section>


    <section>
			<a class="test">
				<img width="800" height="800" src="images/1344895191004_2849138.png">
			</a>
      </section>
     <section>
			<a class="test">
				<img width="600" height="600" src="images/ref_3_matrix.png">
			</a>
      </section>

    <section>
			<a class="test">
				<img width="380" height="426" src="images/honeybadger3.jpg">
			</a>
    </section>

    <section>
			<a class="test">
				<img width="380" height="300" src="images/ML.png">
			</a>
    </section>

				<section>
					<h2>Review Of Probability</h2>
					<p>
						Very short review in order to get everyone on the same page with definitions and intuition before
            we start looking code.
					</p>

					<aside class="notes">
						Oh hey, these are some notes. They'll be hidden in your presentation, but you can see them if you open the speaker notes window (hit 's' on your keyboard).
					</aside>
				</section>
       <section>
	    <section>
						<h2></h2>
						<p>Some Review</p>
						<a class="test">
							<img width="380" height="426" src="images/dice_one.jpg" alt="Two Die">
						</a>
					</section>
				<!-- Example of nested vertical slides -->
					<section>
						<h2></h2>
						<p>Six Outcomes</p>
						<a class="test">
							<img width="671" height="448" src="images/dice_two.png
" alt="Two Die">
						</a>
					</section>
					<section>
						<h2>Discrete Probability Distribution</h2>
						<p>Uniform Multinomial Distribution Over Six Outcomes</p>
						<a class="test" href="http://cornify.com">
							<img width="780" height="826" src="images/figure_1_histogram.png" alt="Unicorn">
						</a>
					</section>
					<section>
						<h2>Demo</h2>
            <pre><code data-trim contenteditable>
import numpy as np
import matplotlib.pyplot as plt


def roll_dice(trails=10, die_throws=100, p_vector=[1/6.]*6):
    my_result = np.random.multinomial(die_throws, p_vector, trails)
    final_counts = map(sum,my_result.transpose())
    return final_counts

d1 = float(1)/6
d2 = float(1)/6
d3 = float(1)/6
d4 = float(1)/6
d5 = float(1)/6
d6 = float(1)/6

p_vector = [d1, d2, d3, d4, d5, d6]
x = roll_dice(1000, 200, p_vector)

            </code></pre>
      <p>
        $\Omega = \{``d_{1}", ``d_{2}",``d_{3}",``d_{4}",``d_{5}",``d_{6}"\}$
      </p>
      <p>
      $X$(samplespace) = {1,2,3,4,5,6}
      </p>
      <p>
        $P(X=x)$ = $\{\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6}\}$
      </p>

					</section>


         <section>
						<h2>Random Variables</h2>
						<p> In the last slide $X(\Omega)$ looked like


  $$
               X(x) = \left\{
     \begin{array}{lr}
       1 & : x = ``d_{1}"\\
       2 & : x = ``d_{2}"\\
       3 & : x = ``d_{3}"\\
       4 & : x = ``d_{4}"\\
       5 & : x = ``d_{5}"\\
       6 & : x = ``d_{6}"
     \end{array}
   \right.

	$$
           </p>

					</section>
                  <section>

<h2>Random Variables</h2>
						<p> If we were interested in the probability of evens and odds, then $X(\Omega)$ looks like


  $$
     X(x) = \left\{
     \begin{array}{lr}
              0 & : x \in \{``d_{1}", ``d_{3}", ``d_{5}"\}\\
              1 & : x \in \{``d_{2}", ``d_{4}", ``d_{6}"\}
     \end{array}
   \right.

	$$
           </p>
<p> Assuming a fair die, $P(X=x)$ = $\{\frac{1}{2},\frac{1}{2}\}$

					</section>

          <section>
						<h2>Random Varibles in Pictures</h2>
						<a href="#/2" class="image">
							<img width="678" height="338" src="images/Mathematics-Introduction-to-Probability-Theory-9.png" alt="Up arrow">
						</a>


					</section>




          <section>
						<h2>Discrete Probability Distribution</h2>
            <p>Any function, $f(x)$, can serve as the probability distribution for a
              discrete random variable $$X$$ if and only if its values</p>

            \begin{aligned} \\
            f(x) \geq 0  \\
            \end{aligned}
            \begin{aligned} \\
          \sum_{x} f(x) = p_{1} + p_{2} + ... + p_{k} = 1 \\
            \end{aligned}


            <p> In the literature $f(x)$ is written $P(X=x)$ </p>
					</section>



         <section>
						<h2>Code</h2>
						<p>That's it for review let's look at some computer languages</p>
					</section>
				</section>
   <section>
				<section>
					<h2>Probabilities</h2>
					<p>
						How do we assign the correct probabilities to events? But wait?! What is probability anyway?
					</p>
				</section>

				<section>
					<h2>Some Perspectives</h2>
					<p>
            <em>"Frequentist methods assume that probability is the long-run frequency of events (hence the bestowed title)."</em>
					</p>
					<p>
            <em>"Bayesians interpret a probability as measure of belief, or confidence, of an event occurring. "</em>
					</p>
          <p>
            <b>Probabilities are elements of a probability distribution.</b>
					</p>
          <small> <a href="http://nbviewer.ipython.org/urls/raw.github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/master/Chapter1_Introduction/Chapter1_Introduction.ipynb">
                    Probabilistic Programming and Bayesian Methods for Hackers </a></small>
				</section>

     				</section>




        <section>
     <section>
					<h2>Automatic Discovery of Instruction set Categories </h2>
					<p>
						Can a computer discover semantic categories of assembly instructions?
					</p>
           <p>
						This tool helps reverse engineer unknown instruction sets.
					</p>

				</section>
				<section>
					<h2>Ngram Models</h2>
					<p>
            n-grams capture a degree of the global properties while being tractable and concise because they represent an (n-1)th-order Markov language model.
           </p>
				</section>


         <section>
					<h2>Language Models</h2>
					<p>
            A language model consists of a ﬁnite set of vocabulary $V = \{ and, push, xor,  \dots \}$, and a function $p(x_{1}, x_{2} \dots, x_{n})$ such that: </p>
          <ul>
            <li>For any $(x_{1}, \dots, x_{n}) \in V^*, p(x_{1}, x_{2} \dots, x_{n}) \ge 0$ </li>
            <li>And $\displaystyle\sum_{(x_{1}, \dots x_{n}) \in V^*} p(x_{1}, x_{2} \dots, x_{n})$ = 1 </li>
          </ul>

           <p> $p(x_{1}, x_{2}, \dots x_{n})$ is just probability distribution over all possible instruction mnemonic sequences given the vocabulary.</p>
             <small><a href="http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/lm.pdf"> Course Notes for COMS w4705: Language Modeling by Michael Collins </a></small>

				</section>

        <section>
					<h2>Learning a language model from data</h2>
					<p>
            Let $c(x_{1}, \dots , x_{n})$ be the number of times that the instruction sequence $x_{1}, \dots x_{n}$ occures, and N be the total number of sequences in the training data.
            <p> $p(x_{1}, x_{2}, \dots x_{n}) = \frac{c(x_{1}, \dots  x_{n})}{N}$ </p>
				</section>


        <section>
					<h2>Terrible idea</h2>
					<p>
           This model will assign a ton of zeros!!
          <p>
          <p>
            Given a training data, how do we learn the function p?
         <p>
				</section>


        <section>
					<h2>The Question</h2>
          <p>
  		<a href="#/2" class="image">
							<img width="500" height="500" src="images/data_generalize.png">
						</a>
            <p>
				</section>

        	<section>
					<h2>Example: bigram model</h2>
					<p>
            <p> jmp xorl popl movl andl pushl pushl pushl pushl pushl pushl pushl pushl </p>
            <p> call hlt pushl movl subl movl movl movl movzbl </p>
            <p> movsbl cmpl je cmpl jg </p>
          <p> $ p(m_{i} | m_{i-1}) = \frac{c(m_{i-1}m_{i})}{\sum_{m_i} c(m_{i-1}m_{i})} $ <p>
            <p> $ p(s)  = \prod_{i=1}^{l+1}p(m_{i}|m_{i-1}) $ </p>
				</section>

     <section>
					<h2>Example: Trigram estimation</h2>
         <p> As an example, our estimate for p(subl | movl,movl) would be </p>
       <p> $\frac{c(movl,movl, subl)}{c(movl,movl)}$ </p>
      		<a class="image">
							<img width="700" height="500" src="images/ConditionalProbability.png" alt="Up arrow">
						</a>
          </section>



         <section>

            <p> jmp xorl popl movl andl pushl pushl pushl pushl pushl pushl pushl</p>
            <p> call hlt pushl movl subl movl movl movl movzbl </p>
            <p> movsbl cmpl je cmpl jg </p>
            <p> p(jmp cmpl pushl) =
            $ p(jmp | \bullet) p(cmpl | jmp) p(pushl | cmpl) p(\bullet | pushl) $<p>
            <p> $ p(s)  = \prod_{i=1}^{4}p(m_{i}|m_{i-1}) $ </p>
				</section>

        <section>
            <p> jmp xorl popl movl andl pushl pushl pushl pushl pushl pushl pushl</p>
            <p> call hlt pushl movl subl movl movl movl movzbl </p>
            <p> movsbl cmpl je cmpl jg </p>
            p(jmp cmpl pushl)
            <p> $ p(jmp | \bullet) p(cmpl | jmp) p(pushl | cmpl) p(\bullet | pushl) $<p>
          <p> $ \frac{c(\bullet,jmp)}{\sum_{m} c(\bullet,m)} \frac{c(jmp,cmpl)}{\sum_{m} c(jmp,w)} \frac{c(cmpl,push1)}{\sum_{w} c(cmpl,w)} \frac{c(push1,\bullet)}{\sum_{w} c(pushl,w)}$
          <p> $ \frac{1}{3}  \frac{0}{1} \frac{0}{1} \frac{1}{5} = 0$ </p>
				</section>

        <section>
        <a href="#/2" class="image">
							<img width="600" height="430" src="images/wat_explained_grande.jpg" alt="Up arrow">
						</a>
        </section>

         <section>
					<h2>Smooooooothing</h2>
					<p>
            $ p(m_{i} | m_{i-1}) = \frac{1 + c(m_{i-1}m_{i})}{\sum_{m_i} 1 + c(m_{i-1}m_{i})} $
					</p>
           <p>
             $ p(m_{i} | m_{i-1}) = \frac{1 + c(m_{i-1}m_{i})}{|V| + \sum_{m_i} c(m_{i-1}m_{i})} $
           </p>
             <p> where $V = \{m : c(m) > 0\} \cup \{UNK\} $ </p>

          <a class="image">
							<img width="300" height="330" src="images/pbco-so-2012-web-lrg.jpg">
						</a>
 <small> <a href="http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">NLP Lunch Tutorial: Smoothing Bill MacCartney</a> </small>
				</section>


        <section>
					<h2>Other Methods to consider </h2>
          <p> Additive smoothing </p>
          <p> Good-Turing estimate </p>
          <p> Jelinek-Mercer smoothing (interpolation) </p>
          <p>  Katz smoothing </p>
          <p> Witten-Bell smoothing </p>
          <p> Absolute discounting </p>
          <p> Kneser-Ney smoothing </p>

				</section>

</section>

<section>

        <section>
          <p><b>Unsupervised Learning of Instruction Categories</b></p>
<p>
<table border="1" bordercolor="#000000" width="100%" cellpadding="3" cellspacing="3">

  <tr>

		<td>000</td>
		<td>call</td>
		<td>2559560</td>
	</tr>
	<tr>
		<td>0010</td>
		<td>lea</td>
		<td>1802186</td>
	</tr>
	<tr>
		<td>00110</td>
		<td>movl</td>
		<td>1836339</td>
	</tr>
	<tr>
		<td>00111</td>
		<td>mov</td>
		<td>10932687</td>
	</tr>
	<tr>
		<td>0100</td>
		<td>je</td>
		<td>1892048</td>
	</tr>
	<tr>
		<td>01010</td>
		<td>jne</td>
		<td>1011392</td>
	</tr>
	<tr>
		<td>0101100</td>
		<td>jg</td>
		<td>61836</td>
	</tr>
	<tr>
		<td>0101101</td>
		<td>jle</td>
		<td>68713</td>
	</tr>
	<tr>
		<td>01011100</td>
		<td>rep</td>
		<td>9697</td>
	</tr>
	<tr>
		<td>0101110100</td>
		<td>cmovns</td>
		<td>1907</td>
	</tr>
	<tr>
		<td>0101110101</td>
		<td>cmovs</td>
		<td>1699</td>
	</tr>
	<tr>
		<td>0101110110</td>
		<td>cmove</td>
		<td>20923</td>
	</tr>
	<tr>
		<td>0101110111</td>
		<td>cmovne</td>
		<td>12415</td>
	</tr>
	<tr>
		<td>010111100</td>
		<td>setne</td>
		<td>12263</td>
	</tr>
	<tr>
		<td>0101111010</td>
		<td>sete</td>
		<td>17697</td>
	</tr>
	<tr>
		<td>010111101100</td>
		<td>setg</td>
		<td>1626</td>
	</tr>

</table>
        </section>

<section>
  <table border="1" bordercolor="#000000" width="100%" cellpadding="3" cellspacing="3">
	<tr>
		<td>010111101101</td>
		<td>setle</td>
		<td>504</td>
	</tr>
	<tr>
		<td>0101111011100</td>
		<td>setl</td>
		<td>452</td>
	</tr>
	<tr>
		<td>0101111011101</td>
		<td>setge</td>
		<td>392</td>
	</tr>
	<tr>
		<td>0101111011110</td>
		<td>setbe</td>
		<td>568</td>
	</tr>
	<tr>
		<td>0101111011111</td>
		<td>setae</td>
		<td>277</td>
	</tr>
	<tr>
		<td>01011111000</td>
		<td>cmovbe</td>
		<td>2939</td>
	</tr>
	<tr>
		<td>010111110010</td>
		<td>cmovae</td>
		<td>1278</td>
	</tr>
	<tr>
		<td>010111110011</td>
		<td>cmovb</td>
		<td>932</td>
	</tr>
	<tr>
		<td>01011111010</td>
		<td>cmovg</td>
		<td>2090</td>
	</tr>
	<tr>
		<td>000111110110</td>
		<td>cmovle</td>
		<td>2274</td>
	</tr>
	<tr>
		<td>0001111101110</td>
		<td>cmovge</td>
		<td>1361</td>
	</tr>
	<tr>
		<td>00011111011110</td>
		<td>cmovl</td>
		<td>1021</td>
	</tr>
 </table>

</section>

  <section>

    <table border="1" bordercolor="#000000" width="100%" cellpadding="3" cellspacing="3">
	<tr>
		<td>1110</td>
		<td>pop</td>
		<td>1663486</td>
	</tr>
	<tr>
		<td>11110</td>
		<td>cmp</td>
		<td>984856</td>
	</tr>
	<tr>
		<td>1111100000</td>
		<td>jp</td>
		<td>34325</td>
	</tr>
	<tr>
		<td>1111100001</td>
		<td>daa</td>
		<td>33944</td>
	</tr>
	<tr>
		<td>1111100010</td>
		<td>ds</td>
		<td>24894</td>
	</tr>
	<tr>
		<td>1111100011</td>
		<td>jno</td>
		<td>136228</td>
	</tr>
	<tr>
		<td>111110011</td>
		<td>jns</td>
		<td>137145</td>
	</tr>
	<tr>
		<td>111110101</td>
		<td>jl</td>
		<td>49303</td>
	</tr>
	<tr>
		<td>111110110</td>
		<td>ja</td>
		<td>190602</td>
	</tr>
	<tr>
		<td>111110111</td>
		<td>jbe</td>
		<td>210968</td>
	</tr>
	<tr>
		<td>1111110</td>
		<td>and</td>
		<td>1357663</td>
	</tr>
 </table>
  </section>



  <section>
    <p> <b> Constructing Variable-Length Code </b></p>
    <p> Every mnemonic is assigned a bit-string.</p>
    <p> If a word's left bits agree with another word's, those words will be in a similar category.</p>
       <a href="#/2" class="image">
							<img width="978" height="306" src="images/prefixcode.png" alt="Up arrow">
						</a>

  </section>


  <section>
    <p> </p>
 <pre><code data-trim contenteditable>
 Initial setup: Place each instruction mnemonic in each cluster.
 And compute the Average Mutual Information (AMI).
 repeat
for each pairs of clusters do
    Merge the pair of clusters temporarily
    Compute the AMI of the collection
end for
Select a pair of cluster with min decrease in AMI
Compute AMI of the new collection
until reach the predefined number of clusters
repeat
Move each mnemonic to the cluster for which the resulting partition
has the greatest AMI until no more increment to AMI
  </code></pre>
    <small> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.935&rep=rep1&type=pdf"> Saeedeh Momtazi and Dietrich Klakow </a> </small>
  </section>


<section>
<p> Set up 1,000 different “instruction categories”, each with one member: the 1,000 most frequent words. </p>
  <p> Consider all $\frac{1000*999}{2}$ ways of collapsing these, and pick the one which minimizes the decrease in the mutual information
  you get when you pass from a system with 1,000 categories to one with 999 categories... </p>
<p> Repeat until you’re done.</p>
  <p> Along the way assign each instruction a bit-string based on the clusters it was in and when they were merged. </p>
<small> <a href="http://hum.uchicago.edu/~jagoldsm/Powerpoint/IBM_Clustering.ppt"> John Goldsmith </a> </small>

</section>

  <section>
        <a href="#/2" class="image">
							<img width="555" height="300" src="images/mutual_info.gif" alt="Up arrow">
						</a>
  $I(X,Y) = H(X) - H(X|Y) = H(X) + H(Y) - H(X,Y) = $
  $\sum_y p(y) \sum_x p(x|y) \log_2 \frac{p(x|y)}{p(x)}$
  $H(X_{1}, \dots, X_{n}) = -\sum_{x_{1}} \dots \sum_{x_{n}}P(x_{1}, \dots, x_{n}) log[P(x_{1}, \dots, x_{n})]$
  $\displaystyle H(X)= - \sum_{i=1}^np(x_i)\log_b p(x_i) $
  </section>

  <section>
<pre><code data-trim contenteditable>
def entropy(counts):
    ps = counts/float(sum(counts))
    ps = ps[nonzero(ps)]
    H = -sum(ps * numpy.log2(ps))

    return H

def mutual_information(counts_xy,counts_x,counts_y)
    H_xy = entropy(counts_xy)
    H_x = entropy(counts_x)
    H_y = entropy(counts_y)

    return H_x + H_y - H_xy
 </code></pre>
  </section>
   <section>


     <a href="#/2" class="image">
			<img width="300" height="293" src="images/toomuch.jpg" alt="Up arrow">
			</a>
   </section>
  <section>
<p>
"When you can measure what you are speaking about, and express it in numbers, you know something about it, when you cannot express it in numbers, your knowledge is of a meager and unsatisfactory kind;
it may be the beginning of knowledge, but you have scarcely, in your thoughts advanced to the stage of science." - Lord Kelvin<p>
 <a href="#/2" class="image">
<img width="300" height="293" src="images/Thomson_5.jpeg" alt="Up arrow">
</a>



</section>



 </section>

  <section>
       <a href="#/2" class="image">
							<img width="455" height="378" src="images/caredo.jpg" alt="Up arrow">
						</a>

  </section>



<p style="font-family:verdana,arial,sans-serif;font-size:10px;"></p>

        <section>Automatic grouping of similar objects

          <p><b>Often joint optimization of two quality criteria</b></p>
          <ul>
            <li>Keep Similar object in the same group</li>
            <li>Keep different objects in different groups</li>
          </ul>
   <a href="#/2" class="image">
							<img width="978" height="306" src="images/types_clusters.png" alt="Up arrow">
						</a>
        <small> <a href="http://filepool.informatik.uni-goettingen.de/gcms/sec/2013-mlsec/slides/08-cluster.pdf">Clustering of Malicious Software</a></small>

			</section>


           <section>K-means clustering

          <p></p>
          <ul>
           <li>Partitioning of data using k centroids (mean vectors)</li>
            <li>Numerous variants: hard/soft, spectral, k-medoids</li>
           <li>Compact representation of clusters by centroids</li>


          </ul>
   <a href="#/2" class="image">
							<img width="978" height="506" src="images/random.gif" alt="Up arrow">
						</a>

			</section>


        <section>
					<h2>Kmeans Example Code</h2>

       <pre>
      <code data-trim contenteditable>
from sklearn.cluster import KMeans
kmeans = KMeans(50, random_state=8)
kmeans_out = kmeans.fit(X)
print k_means.labels_[::10]
        </code>
      </pre>
          <p> Optimization problem underlying k-means clustering </p>
          Given your data vectors, $(x_{1}, \dots, x_{n})$ group into k clusters
          $C = \{C_{1}, \dots, C_{k}\}$ so that the following is minimized
          $ \underset{C}{\operatorname{argmin}}  \sum_{i=1}^k \sum_{x_{j} \in C_{i}} || x_{j} - \mu_{i} ||^2 $
          where
          $ \mu_{i}$ is the mean of points in $C_{i}$.
          </section>



        <section>
					<h2>Demo</h2>

          						Try it out!
        </section>


             <section>
					<p> Properties of k-means clustering</p>
				<ul>
          <li>Need to know or guess the number of clusters before hand</li>
          <li>The clusters are convex due to the optmization problem</li>
          </ul>
				   <a href="#/2" class="image">
							<img width="978" height="506" src="images/DBSCAN-density-data.svg" alt="Up arrow">
						</a>

				</section>

<section>
                <section>
					<h2>Malheur</h2>
					<p>
            Automatic Analysis of Malware Behavior
          </p>
        <a href="http://www.mlsec.org/malheur/" class="image">
							<img width="163" height="170" src="images/malheur_logo.png">
				</a>
				</section>

        <section>
					<h2>Dimensionality Reduction</h2>
					<p>
            We start with a similarity matrix and then assign for each item a location in a low-dimensional space.
					</p>

			<a class="test">
				<img width="600" height="600" src="images/ref_matrix.png">
			</a>
      </section>


        <section>

			<a class="test">
				<img width="600" height="600" src="images/test-output.chaos.png">
			</a>
				</section>

          <section>
		<a class="test">
				<img width="300" height="297" src="images/38881514.jpg">
			</a>
				</section>


          <section>
		<a class="test">
				<img width="658" height="430" src="images/project.png">
			</a>
				</section>
           <section>
             <p> Your Turn </p>
		<a class="test">
				<img width="300" height="225" src="images/real_math.jpg">
			</a>
				</section>


     				</section>


                  <section>

          <section>
			<h2>Future Work</h2>
				<ul>
         <li>Program Similarity Using Programming Language Theory</li>
         <li>Better Visualizations</li>
         <li>Statistical Decomplication</li>
        </ul>


				</section>
    <section>
			<h2>Better Semantics</h2>
        <ul>
          <li><a href="http://www.jakstab.org/"> Jakstab </a></li>
         <li><a href="https://github.com/neuromancer/SEA">Symbolic Exploit Assistant</a></li>
         <li><a href="https://github.com/bitblaze-fuzzball/fuzzball"> FuzzBALL Binary Symbolic Execution </a></li>
          <li><a href="http://code.google.com/p/avalanche/">Avalanche</a></li>
        </ul>
       </section>
    <section>
			<h2>Better Visualizations</h2>
    <a class="test">
				<img width="1156" height="479" src="images/labs.png">
			</a>
       </section>


        <section>

                     <a class="test">
				<img width="1156" height="479" src="images/labs2.png">
			</a>
      </section>

      <section>
    <a class="test">
				<img width="973" height="711" src="images/ether.png">
			</a>
       </section>



				</section>




				<section>
			<section>
					<h3>References</h3>
      <ul>
         <li><a href="https://www.blackhat.com/us-13/briefings.html#Hanif">BinaryPig - Scalable Malware Analytics in Hadoop</a></li>
         <li><a href="http://www.hum.uchicago.edu/jagoldsm/Papers/probabilityProofs.pdf"> Probability for Linguists by John Goldsmith</a> </li>
        <li><a href="http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/lm.pdf"> Course Notes for COMS w4705: Language Modeling by Michael Collins </a></li>
         <li><a href="http://web.mit.edu/6.863/www/fall2012/readings/ngrampages.pdf"> Speech and Language Processing by Daniel Jurafsky & James H. Martin.</a></li>
         <li><a href="http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf">NLP Lunch Tutorial: Smoothing Bill MacCartney</a> </li>
         <li><a href="http://hum.uchicago.edu/~jagoldsm/Powerpoint/IBM_Clustering.ppt">IBM Clustering: after Brown et al by John Goldsmith</a> </li>
         <li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.157.935&rep=rep1&type=pdf">A Word Clustering Approach for Language Model-based Sentence Retrieval in Question Answering Systems by Saeedeh Momtazi and Dietrich Klakow</a></li>
         <li><a href="http://acl.ldc.upenn.edu/J/J92/J92-4003.pdf">Class-Based n-gram Models of Natural Language by Brown, Peter F., Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai</a></li>
         <li><a href="http://filepool.informatik.uni-goettingen.de/gcms/sec/2013-mlsec/slides/08-cluster.pdf">Clustering of Malicious Software</a></li>
         <li><a href="http://scikit-learn.org/stable/modules/clustering.html">Scikit-Learn Clustering</a></li>
         <li><a href="http://www.mlsec.org/malheur/">Malheur</a></li>
        </ul>
      </section>

   			<section>
					<h3>References Continuted</h3>
      <ul>
         <li><a href="http://homepage.tudelft.nl/19j49/t-SNE.html">t-Distributed Stochastic Neighbor Embedding</a></li>
         <li><a href="http://www.shmoocon.org/2012/presentations/Danny_Quist-3dmalware-shmoocon2012.pdf">Malware Visualization in 3D by Danny Quist</a></li>
         <li><a href="http://www.cse.ohio-state.edu/~raghu/teaching/CSE5544/Visweek2012/vizsec/33-Saxe.pdf">Visualization of Shared System Call Sequence Relationships in Large Malware Corpora by Josh Saxe, David Mentis, and Chris Greamo</a></li>
        </ul>
      </section>
    </section>


				<section>
					<h3>We're Hiring! Rafael.Turner AT invincea.com</h3>
          <p><a href="#/0">Go Back</a></p>
				</section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>
    	<script type="text/x-mathjax-config">
			MathJax.Hub.Config({
				tex2jax: {
				  inlineMath: [ ['$','$'] ],
				  displayMath: [ ['$$','$$'] ],
				  processEscapes: true,
				},
				messageStyle: "none",
			});
		</script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML', async: true },
					{ src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; } }
					// { src: 'plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

	</body>
</html>
